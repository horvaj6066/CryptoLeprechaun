{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction Update - called at 5PM after BTC price update to run the prediction model with the new price\n",
    "#then it saves it to a csv file that is used to update the charts \n",
    "#based on - https://github.com/JerryWei03/DeepCryptocurrency/blob/master/A%20Deep%20Learning%20Approach%20to%20Predicting%20Cryptocurrency%20Prices.ipynb\n",
    "\n",
    "\n",
    "#3 dimensional array as input to LSTM network - 1st is batch size, 2nd is time steps, and third is sequency length\n",
    "from keras.models import load_model \n",
    "from keras.models import model_from_json\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#load up and format the data for the model \n",
    "sequence_length=30\n",
    "y_column=3\n",
    "current_model= \"0.5631067961165048\"\n",
    "path = \"C:\\\\Users\\\\Jon P Horvath\\\\Documents\\\\Bootcamp_HW\\\\Project_Work\\\\Machine_Learning\\\\cryptoleprechaun\\\\\"\n",
    "file = \"price_predict_and_hist_data.csv\"\n",
    "#upload from the CSV, first one is simple file                         \n",
    "#raw_data = pd.read_csv(\"C:\\\\Users\\\\Jon P Horvath\\\\Documents\\\\Bootcamp_HW\\\\Project_Work\\\\Machine_Learning\\\\3Y_BTC.csv\", dtype = float).values\n",
    "#print(raw_data.shape)\n",
    "#raw_data=raw_data[:,1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1072, 10)\n",
      "data length shd be 1061 +-1: 1072\n",
      "start= 938\n",
      "end = dr.shape[0] + 1 1043\n",
      "unnormalized bases shape:\n",
      "(104, 1)\n",
      "length training data 938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#here we are going to load up the price predict file, isolate the 7 fields and run the program\n",
    "raw_data = pd.read_csv(path+file, dtype = float).values\n",
    "print(raw_data.shape)\n",
    "raw_data=raw_data[:,1:7] #drops date and last 3 columns\n",
    "\n",
    "#Change all zeros to the number before the zero occurs\n",
    "for x in range(0, raw_data.shape[0]):\n",
    "    for y in range(0, raw_data.shape[1]):\n",
    "        if(raw_data[x][y] == 0):\n",
    "            raw_data[x][y] = raw_data[x-1][y]\n",
    "\n",
    "#convert the file to a list\n",
    "data = raw_data.tolist()\n",
    "\n",
    "#Convert the data to a 3D array (a x b x c) \n",
    "#Where a is the number of days, b is the window size, and c is the number of features in the data file\n",
    "result = []\n",
    "for index in range(len(data) - sequence_length):\n",
    "    result.append(data[index: index + sequence_length])\n",
    "\n",
    "print(\"data length shd be 1061 +-1:\", len(data)) #1060 makes sense \n",
    "\n",
    "#Normalizing data by going through each window\n",
    "#Every value in the window is divided by the first value in the window, and then 1 is subtracted\n",
    "d0 = np.array(result)\n",
    "dr = np.zeros_like(d0)\n",
    "dr[:,1:,:] = d0[:,1:,:] / d0[:,0:1,:] - 1\n",
    "#print(\"d0 shape:\",d0.shape)\n",
    "\n",
    "#Keeping the unnormalized prices for Y_test\n",
    "#Useful when graphing bitcoin price over time later\n",
    "start_num = d0.shape\n",
    "start_num=0.9*start_num[0]\n",
    "\n",
    "#START needs to EQUAL shape of xtrain, seems to match up to length of training data\n",
    "start = round(start_num) # was 910, 927, 933 -REALLY important that this is x-shape split_line #split_line = round(0.9 * dr.shape[0]) WAS hard CODED!!!\n",
    "#start = 933 # was 910 -REALLY important that this is x-shape split_line #split_line = round(0.9 * dr.shape[0]) WAS hard CODED!!!\n",
    "end = int(dr.shape[0] + 1)\n",
    "print(\"start=\", start)\n",
    "print(\"end = dr.shape[0] + 1\",end)\n",
    "\n",
    "#unnormalized_bases = d0[start:end,0:1,20] #20 was where the bitcoin price was in original file \n",
    "unnormalized_bases = d0[start:end,0:1, y_column] #this is the first problem line \n",
    "print(\"unnormalized bases shape:\") \n",
    "print(unnormalized_bases.shape) \n",
    "\n",
    "#Splitting data set into training (First 90% of data points) and testing data (last 10% of data points)\n",
    "split_line = round(0.9 * dr.shape[0])  #split between training and predict\n",
    "training_data = dr[:int(split_line), :]\n",
    "print(\"length training data\", len(training_data))\n",
    "#Shuffle the data -WHY DO WE SHUFFLE THE DATA IN A TIME SERIES ???\n",
    "np.random.shuffle(training_data)\n",
    "\n",
    "#Training Data\n",
    "X_train = training_data[:, :-1]\n",
    "Y_train = training_data[:, -1]\n",
    "print()\n",
    "\n",
    "Y_train = Y_train[:, y_column] #the 20 is because bitcoin price was 20th column in his data\n",
    "\n",
    "#Testing data\n",
    "X_test = dr[int(split_line):, :-1] #split line is just the dividing point for the training data\n",
    "Y_test = dr[int(split_line):, (sequence_length-1), :] #was 49 middle and was using 50 seq length \n",
    "Y_test = Y_test[:, y_column] #the 20 is because bitcoin price was 20th column in his data\n",
    "\n",
    "#Get the day before Y_test's price\n",
    "Y_daybefore = dr[int(split_line):, (sequence_length-2), :] #was 48 and was using 50 sequence length \n",
    "Y_daybefore = Y_daybefore[:, y_column] #the 20 is because bitcoin price was 20th column in his data\n",
    "\n",
    "#Get window size and sequence length\n",
    "sequence_length = sequence_length\n",
    "window_size = sequence_length - 1 #because the last value is reserved as the y value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(938, 29, 6)\n",
      "937.8000000000001\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(start_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 29, 58)            8352      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 29, 58)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 29, 116)           54288     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 29, 116)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 58)                33872     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 59        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 96,571\n",
      "Trainable params: 96,571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Model reconstruction from JSON file\n",
    "with open('C:\\\\Users\\\\Jon P Horvath\\\\Documents\\\\Bootcamp_HW\\\\Project_Work\\\\Machine_Learning\\\\'+current_model+'.json', 'r') as f:\n",
    "    model = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "model.load_weights('C:\\\\Users\\\\Jon P Horvath\\\\Documents\\\\Bootcamp_HW\\\\Project_Work\\\\Machine_Learning\\\\'+current_model+'.h5')\n",
    "print(model.summary())\n",
    "\n",
    "y_predict = model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest prediction: 9539.28515625\n"
     ]
    }
   ],
   "source": [
    "#Create empty 2D arrays to store unnormalized values\n",
    "real_y_test = np.zeros_like(Y_test)\n",
    "real_y_predict = np.zeros_like(y_predict)\n",
    "#unnormalize\n",
    "#Fill the 2D arrays with the real value and the predicted value by reversing the normalization process\n",
    "for i in range(Y_test.shape[0]):\n",
    "    y = Y_test[i]\n",
    "    predict = y_predict[i]\n",
    "    real_y_test[i] = (y+1)*unnormalized_bases[i] \n",
    "    real_y_predict[i] = (predict+1)*unnormalized_bases[i]\n",
    "predict=float(real_y_predict[-1])\n",
    "print(\"latest prediction:\", predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data at very end of predict right before it's saved for js \n",
      "           date      open      high       low     close  unit_volume  \\\n",
      "0     20160924    602.96    604.58    602.04    602.63    58.675307   \n",
      "1     20160925    602.75    603.38    599.71    600.83    56.551437   \n",
      "2     20160926    600.81    608.14    600.35    608.04    97.286034   \n",
      "3     20160927    608.02    608.25    604.11    606.17    81.532243   \n",
      "4     20160928    606.24    606.59    604.61    604.73    80.569180   \n",
      "5     20160929    605.02    606.82    604.85    605.69    91.892882   \n",
      "6     20160930    605.72    609.73    604.14    609.73    92.044676   \n",
      "7     20161001    609.93    615.24    609.93    613.98    91.789635   \n",
      "8     20161002    613.95    614.01    609.68    610.89    64.250192   \n",
      "9     20161003    610.97    612.57    610.46    612.13    76.451571   \n",
      "10    20161004    612.05    612.05    609.48    610.20    81.615208   \n",
      "11    20161005    610.22    613.81    609.62    612.51   111.145131   \n",
      "12    20161006    612.47    613.82    611.47    613.02    92.675769   \n",
      "13    20161007    612.61    617.91    611.82    617.12   103.823244   \n",
      "14    20161008    617.34    619.85    617.34    619.11    68.398023   \n",
      "15    20161009    619.17    619.20    616.61    616.75    63.629347   \n",
      "16    20161010    616.82    621.32    616.20    618.99   109.018084   \n",
      "17    20161011    619.24    642.08    618.50    641.07   161.589218   \n",
      "18    20161012    640.87    641.34    635.97    636.19   145.192788   \n",
      "19    20161013    636.03    638.83    635.03    636.79    96.767694   \n",
      "20    20161014    637.01    641.28    637.01    640.38    90.797027   \n",
      "21    20161015    640.31    642.10    637.39    638.65    61.121741   \n",
      "22    20161016    639.08    642.90    638.90    641.63    62.805823   \n",
      "23    20161017    641.82    642.33    638.66    639.19    90.839344   \n",
      "24    20161018    639.41    640.74    636.00    637.96   102.744216   \n",
      "25    20161019    638.13    638.87    628.01    630.52   110.038850   \n",
      "26    20161020    630.66    631.92    628.26    630.86    90.285166   \n",
      "27    20161021    630.83    634.09    630.69    632.83    88.413950   \n",
      "28    20161022    633.14    658.20    632.85    657.29   119.515733   \n",
      "29    20161023    657.62    661.13    653.89    657.07    82.905322   \n",
      "...        ...       ...       ...       ...       ...          ...   \n",
      "1042  20190802  10402.04  10657.95  10371.01  10518.17  1662.750657   \n",
      "1043  20190803  10519.28  10946.78  10503.50  10821.73  1418.690455   \n",
      "1044  20190804  10821.63  11009.21  10620.28  10970.18  1506.893669   \n",
      "1045  20190805  10960.74  11895.09  10960.74  11805.65  2022.420522   \n",
      "1046  20190806  11811.55  12273.82  11290.73  11478.17  2059.135529   \n",
      "1047  20190807  11476.19  12036.99  11433.70  11941.97  1858.570122   \n",
      "1048  20190808  11954.04  11979.42  11556.17  11966.41  1628.023085   \n",
      "1049  20190809  11953.47  11970.46  11709.75  11862.94  1545.990282   \n",
      "1050  20190810  11861.56  11915.66  11323.90  11354.02  1596.382202   \n",
      "1051  20190811  11349.74  11523.58  11248.29  11523.58  1368.877685   \n",
      "1052  20190812  11528.19  11528.19  11320.95  11382.62  1198.950525   \n",
      "1053  20190813  11385.05  11420.05  10830.33  10895.83  1530.998881   \n",
      "1054  20190814  10889.49  10889.56  10028.14  10051.70  1988.801725   \n",
      "1055  20190815  10038.42  10437.41   9675.32  10311.55  2220.724826   \n",
      "1056  20190816  10319.42  10524.35   9855.48  10374.34  1949.830745   \n",
      "1057  20190817  10358.72  10452.62  10086.70  10231.74  1346.597518   \n",
      "1058  20190818  10233.01  10487.07  10119.09  10345.81  1256.529346   \n",
      "1059  20190819  10350.28  10916.05  10313.20  10916.05  1469.237005   \n",
      "1060  20190820  10916.35  10947.04  10618.96  10763.23  1398.565503   \n",
      "1061  20190821  10764.57  10798.73   9962.72  10138.05  1920.791944   \n",
      "1062  20190822  10142.52  10233.00   9831.46  10131.06  1687.632771   \n",
      "1063  20190823  10136.31  10442.44  10078.19  10407.97  1501.447822   \n",
      "1064  20190824  10407.64  10418.02   9982.30  10159.96  1520.776721   \n",
      "1065  20190825  10160.74  10304.62  10008.79  10138.52  1396.047610   \n",
      "1066  20190826  10126.30  10512.33  10126.30  10370.82  1777.935986   \n",
      "1067  20190827  10372.83  10381.33  10087.30  10185.50  1449.375043   \n",
      "1068  20190828  10203.43  10279.37   9716.66   9754.42  1804.698826   \n",
      "1069  20190829   9756.79   9756.79   9421.63   9510.20  1792.378552   \n",
      "1070  20190830   9514.84   9656.12   9428.30   9598.17  1416.443341   \n",
      "1071  20190831   9597.54   9673.22   9531.80   9630.66  1189.410323   \n",
      "\n",
      "      bitmex_funding       predict    actual  profit  \n",
      "0           0.000348           NaN       NaN     NaN  \n",
      "1           0.000273           NaN       NaN     NaN  \n",
      "2           0.002010           NaN       NaN     NaN  \n",
      "3           0.004182           NaN       NaN     NaN  \n",
      "4           0.000249           NaN       NaN     NaN  \n",
      "5           0.001290           NaN       NaN     NaN  \n",
      "6           0.002031           NaN       NaN     NaN  \n",
      "7           0.007806           NaN       NaN     NaN  \n",
      "8           0.000264           NaN       NaN     NaN  \n",
      "9           0.001368           NaN       NaN     NaN  \n",
      "10          0.000237           NaN       NaN     NaN  \n",
      "11          0.001611           NaN       NaN     NaN  \n",
      "12          0.000219           NaN       NaN     NaN  \n",
      "13          0.002334           NaN       NaN     NaN  \n",
      "14          0.002004           NaN       NaN     NaN  \n",
      "15          0.000180           NaN       NaN     NaN  \n",
      "16          0.007485           NaN       NaN     NaN  \n",
      "17          0.000123           NaN       NaN     NaN  \n",
      "18          0.002720           NaN       NaN     NaN  \n",
      "19          0.000201           NaN       NaN     NaN  \n",
      "20          0.001299           NaN       NaN     NaN  \n",
      "21          0.005160           NaN       NaN     NaN  \n",
      "22         -0.000504           NaN       NaN     NaN  \n",
      "23          0.000378           NaN       NaN     NaN  \n",
      "24         -0.000231           NaN       NaN     NaN  \n",
      "25          0.002160           NaN       NaN     NaN  \n",
      "26         -0.002100           NaN       NaN     NaN  \n",
      "27          0.002190           NaN       NaN     NaN  \n",
      "28          0.000216           NaN       NaN     NaN  \n",
      "29          0.007374           NaN       NaN     NaN  \n",
      "...              ...           ...       ...     ...  \n",
      "1042        0.000100  10211.048830  10518.17 -303.56  \n",
      "1043        0.000100  10559.129880  10821.73 -148.45  \n",
      "1044        0.000100  10516.148440  10970.18 -835.47  \n",
      "1045        0.000583  10794.734380  11805.65  327.48  \n",
      "1046        0.001156  10941.205080  11478.17 -463.80  \n",
      "1047        0.000667  11505.773440  11941.97  -24.44  \n",
      "1048        0.000911  11809.875000  11966.41  103.47  \n",
      "1049        0.001156  12042.993160  11862.94 -508.92  \n",
      "1050        0.000619  11627.290040  11354.02  169.56  \n",
      "1051        0.000100  11419.421880  11523.58  140.96  \n",
      "1052        0.000122  11375.984380  11382.62  486.79  \n",
      "1053        0.000295  11092.143550  10895.83 -844.13  \n",
      "1054        0.000100  11338.754880  10051.70  259.85  \n",
      "1055       -0.000085  10631.919920  10311.55   62.79  \n",
      "1056       -0.000138  10162.749020  10374.34  142.60  \n",
      "1057        0.000100  10441.336910  10231.74  114.07  \n",
      "1058        0.000100  10707.261720  10345.81  570.24  \n",
      "1059        0.000100  10570.783200  10916.05  152.82  \n",
      "1060        0.000100  10448.672850  10763.23  625.18  \n",
      "1061        0.000100  11130.365230  10138.05   -6.99  \n",
      "1062        0.000100  11282.456050  10131.06  276.91  \n",
      "1063        0.000100  10779.228520  10407.97 -248.01  \n",
      "1064        0.000098  10623.011720  10159.96  -21.44  \n",
      "1065        0.000100  10547.328130  10138.52 -232.30  \n",
      "1066        0.000157  10364.247070  10370.82  185.32  \n",
      "1067        0.000100  10887.222660  10185.50 -431.08  \n",
      "1068        0.000100  10142.807620   9754.42 -244.22  \n",
      "1069        0.000010  10053.078130   9510.20   87.97  \n",
      "1070       -0.000251   9319.039063   9598.17  -32.49  \n",
      "1071        0.000100   9539.285156   9630.66     NaN  \n",
      "\n",
      "[1072 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv(path+file)\n",
    "raw_data.iloc[-1,7] = predict #add the latest prediction\n",
    "raw_data.iloc[-1,8] = raw_data.iloc[-1,4] #make the latest actual = to the close\n",
    "#lets do profit calc, first figure out if long or short\n",
    "if(raw_data.iloc[-2,7] >= raw_data.iloc[-2,8]):  #this is a BUY prediction ie prediction is higher\n",
    "    raw_data.iloc[-2,9] = raw_data.iloc[-1,8] - raw_data.iloc[-2,8] # diff = actuallast - actual t-2 \n",
    "\n",
    "else:\n",
    "    raw_data.iloc[-2,9] =  raw_data.iloc[-2,8]- raw_data.iloc[-1,8]  #diff = actual t-2 - actual last   \n",
    "\n",
    "print(\"raw data at very end of predict right before it's saved for js \\n\",raw_data)\n",
    "raw_data.set_index('date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.to_csv(path+file)  #save to csv *******************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
